{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c8111aa",
   "metadata": {},
   "source": [
    "**请填写你的姓名与学号**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e19bb988",
   "metadata": {},
   "outputs": [],
   "source": [
    "name=\"康凯\"\n",
    "student_id=\"23009200942\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375ae438",
   "metadata": {},
   "source": [
    "# 线性回归模型编程练习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acfeda36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['environment.yml', 'fit.png', 'linear-regression.ipynb', 'PRML_LR_data.txt', 'scatter.png']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir('.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6b227f2",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'ls' �����ڲ����ⲿ���Ҳ���ǿ����еĳ���\n",
      "���������ļ���\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ������ E �еľ��� �¼Ӿ�\n",
      " �������к��� 0007-73B7\n",
      "\n",
      " e:\\ai\\�Լ�����ѧ\\python\\dl,ml\\PRML-HW25F01 ��Ŀ¼\n",
      "\n",
      "\n",
      " e:\\ai\\�Լ�����ѧ\\python\\dl,ml\\PRML-HW25F01 ��Ŀ¼\n",
      "\n",
      "\n",
      " e:\\ai\\�Լ�����ѧ\\python\\dl,ml\\PRML-HW25F01 ��Ŀ¼\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "�Ҳ����ļ�\n"
     ]
    }
   ],
   "source": [
    "# 查看个人持久化工作区文件\n",
    "#!ls\n",
    "!dir  # Windows 环境"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedeb9d1",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 线性回归模型\n",
    "\n",
    "回归问题是非常常见的一类问题，目的是寻找变量之间的关系。比如要从数据中寻找房屋面积与价格的关系，年龄和身高的关系，气体压力和体积的关系等等。而机器学习要做的正是要让机器自己来学习这些关系，并为对未知的情况做出预测。\n",
    "\n",
    "对于线性回归，假设变量之间的关系是线性的，即：\n",
    "$$h_{\\theta}(x)= \\theta_{0} + \\theta_{1} x$$\n",
    "其中 $\\pmb{\\theta}$ 就是学习算法需要学习的参数，在线性回归的问题上，就是$\\theta_{1}$和$\\theta_{0}$，而 $x$ 是我们对于问题所选取的特征，也即输入。$h$表示算法得到的映射。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a365f6",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 代价函数的表示\n",
    "\n",
    "为了找到这个算法中合适的参数，我们需要制定一个标准。一般而言算法拟合出来的结果与真实的结果误差越小越好，试想一下如果算法拟合出来的结果与真实值的误差为零，那么就是说算法完美地拟合了数据。所以可以根据“真实值与算法拟合值的误差”来表示算法的“合适程度”。在线性回归中，我们经常使用最小二乘的思路构建代价函数：\n",
    "$$J(\\pmb{\\theta}) = \\frac{1}{2n}\\sum_{i=1}^{n} \\Big( h_{\\theta}(x^{(i)}) - y^{(i)} \\Big)^2$$\n",
    "这里 $h_{\\theta}(x^{(i)})$ 由假设模型得出。对线性回归任务，代价函数可以展开为：\n",
    "$$J(\\pmb{\\theta}) = ???$$\n",
    "误差函数的值越小，则代表算法拟合结果与真实结果越接近。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e93cbf8",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 梯度下降\n",
    "\n",
    "梯度下降算法沿着误差函数的反向更新$\\theta$的值，直到代价函数收敛到最小值。梯度下降算法更新$\\theta_i$的方法为：\n",
    "$$\\theta_{j} = \\theta_{j} - \\alpha\\frac{\\partial }{\\partial \\theta_{j}}J(\\pmb{\\theta}), \\qquad j=0, 1$$\n",
    "其中 $\\alpha$表示学习率。对于线性回归的的参数，可以根据代价函数求出其参数更新公式：\n",
    "$$\\begin{split}\\frac{\\partial J}{\\partial \\theta_{0} } &= ???,\\\\\n",
    "\\frac{\\partial J}{\\partial \\theta_{1} } &= ???.\\end{split}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0ce2bf",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 代码实现\n",
    "现在让我们开始动手实现，首先让我们回顾一下numpy和matplotlib："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7272aaab",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def warm_up_exercise():\n",
    "    \"\"\"热身练习\"\"\"\n",
    "    A = None\n",
    "    # ====================== 你的代码 ==========================\n",
    "    # 在下面加入你的代码，使程序返回一个 5x5 的单位矩阵\n",
    "    # =========================================================\n",
    "    return A\n",
    "\n",
    "# 当你的实现正确时，下面会输出一个单位矩阵：\n",
    "print(warm_up_exercise())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d7f38e",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "你需要实现绘制数据集中图像的函数，当你的实现|正确时，你应该会得到如下的图像：\n",
    "![散点图](./scatter.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108a070c",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_data(x, y):\n",
    "    \"\"\"绘制给定数据x与y的图像\"\"\"\n",
    "    plt.figure()\n",
    "    # ====================== 你的代码 ==========================\n",
    "    # 绘制x与y的图像\n",
    "    # 使用 matplotlib.pyplt 的命令 plot, xlabel, ylabel 等。\n",
    "    # 提示：可以使用 'rx' 选项使数据点显示为红色的 \"x\"，\n",
    "    #       使用 \"markersize=8, markeredgewidth=2\" 使标记更大\n",
    "\n",
    "    # 给制数据\n",
    "\n",
    "    # 设置y轴标题为 'Profit in $10,000s'\n",
    "    \n",
    "    # 设置x轴标题为 'Population of City in 10,000s'\n",
    "    \n",
    "    # =========================================================\n",
    "    plt.show()\n",
    "    \n",
    "# 让我们测试一下你的实现是否正确\n",
    "# 从txt中加载数据\n",
    "print('Plotting Data ...\\n')\n",
    "data = np.loadtxt('./PRML_LR_data.txt', delimiter=',')\n",
    "x, y = data[:, 0], data[:, 1]\n",
    "\n",
    "# 绘图\n",
    "plot_data(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4290fa56",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "现在运用所学的知识，对上述数据利用线性回归进行拟合。首先我们对要学习的参数和数据做一个准备:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5657ec60",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Add a column of ones to x\n",
    "m = len(y)\n",
    "X = np.ones((m, 2))\n",
    "X[:, 1] = data[:, 0]\n",
    "\n",
    "# initialize fitting parameters\n",
    "theta = np.zeros((2, 1))\n",
    "\n",
    "# Some gradient descent settings\n",
    "iterations = 1500\n",
    "alpha = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c3241b",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "计算初始误差函数的值，你需要实现误差函数的计算："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9967ebb",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_cost(X, y, theta):\n",
    "    \"\"\"计算线性回归的代价。\"\"\"\n",
    "    m = len(y)\n",
    "    J = 0.0\n",
    "    # ====================== 你的代码 ==========================\n",
    "    # 计算给定 theta 参数下线性回归的代价\n",
    "    # 请将正确的代价赋值给 J\n",
    "    \n",
    "    # =========================================================\n",
    "    return J\n",
    "\n",
    "# compute and display initial cost\n",
    "# Expected value 32.07\n",
    "J0 = compute_cost(X, y, theta)\n",
    "print(J0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f9662f",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "现在你验证了代价计算的正确性，接下来就需要实现最核心的部分：梯度下降。在实现这一部分之前，确定你理解了上述各种变量及其表示。你需要完成梯度下降的核心代码部分："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff259dbc",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, theta, alpha, num_iters):\n",
    "    \"\"\"执行梯度下降算法来学习参数 theta。\"\"\"\n",
    "    m = len(y)\n",
    "    J_history = np.zeros((num_iters,))\n",
    "\n",
    "    for iter in range(num_iters):\n",
    "        # ====================== 你的代码 ==========================\n",
    "        # 计算给定 theta 参数下线性回归的梯度，实现梯度下降算法\n",
    "        # =========================================================\n",
    "        # 将各次迭代后的代价进行记录\n",
    "        J_history[iter] = compute_cost(X, y, theta)\n",
    "\n",
    "    return theta, J_history\n",
    "\n",
    "# run gradient descent\n",
    "# Expected value: theta = [-3.630291, 1.166362]\n",
    "theta, J_history = gradient_descent(X, y, theta,\n",
    "                                    alpha, iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99771d7",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "为了验证梯度下降方法实现的正确性，你需要把学习的到的直线绘制出来，确定你的实现是否正确。前面你已经绘制了数据集中的点，现在你需要在点的基础上绘制一条直线，如果你的实现正确，那么得到的图像如下：\n",
    "\n",
    "![拟合结果](./fit.png)\n",
    "\n",
    "现在你已经正确实现了线性回归，你可能会对误差函数的优化过程比较好奇。为了更好地理解这个过程，你可以将损失函数的图像绘制出来。为此你需要将需要优化的参数的各个取值时误差函数的取值在图像上绘制出来，以下代码需要你进行填写。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccd5eb9",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_visualize_cost(X, y, theta_best):\n",
    "    \"\"\"可视化代价函数\"\"\"\n",
    "\n",
    "    # 生成参数网格\n",
    "    theta0_vals = np.linspace(-10, 10, 101)\n",
    "    theta1_vals = np.linspace(-1, 4, 101)\n",
    "    t = np.zeros((2, 1))\n",
    "    J_vals = np.zeros((101, 101))\n",
    "    for i in range(101):\n",
    "        for j in range(101):\n",
    "            # =============== 你的代码 ===================\n",
    "            # 加入代码，计算 J_vals 的值\n",
    "            \n",
    "            # ===========================================\n",
    "\n",
    "    plt.figure()\n",
    "    plt.contour(theta0_vals, theta1_vals, J_vals,\n",
    "                levels=np.logspace(-2, 3, 21))\n",
    "    plt.plot(theta_best[0], theta_best[1], 'rx',\n",
    "             markersize=8, markeredgewidth=2)\n",
    "    plt.xlabel(r'$\\theta_0$')\n",
    "    plt.ylabel(r'$\\theta_1$')\n",
    "    plt.title(r'$J(\\theta)$')\n",
    "  \n",
    "\n",
    "plot_visualize_cost(X, y, theta)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a035acab",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "在梯度更新时，我们保留了代价的历史信息。在参数的学习过程中，代价函数的变化过程你也可以作一个图来查看。观察最后得到的$J(\\theta)$的图像以及代价的变化过程，可以加深你的理解。在梯度下降的迭代中，我们设置终止条件为完成了固定的迭代次数，但是在迭代次数完成时，由于学习率等参数的设置，可能得到的参数并不是使得代价最低的值。你可以通过观察代价函数的变化过程，想办法调整学习率等参数或者改进程序，使得参数的取值为搜索到的最优结果。改进`plot_visualize_cost`绘制模型迭代优化历史。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478d7b36",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def plot_visual_history(X, y, ???):\n",
    "    plt.figure()\n",
    "    # =============== 你的代码 ===================\n",
    "            \n",
    "    # ===========================================\n",
    "    plt.xlabel(r'$\\theta_0$')\n",
    "    plt.ylabel(r'$\\theta_1$')\n",
    "    plt.title(r'$J(\\theta)$')\n",
    "    \n",
    "\n",
    "plot_visual_history()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b865f338",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 进阶\n",
    "在实现中，你可能采取了像上面公式中给出的结果一样逐个样本计算代价函数，或者在梯度下降的更新时也采用了逐个样本计算的方式。但事实上，你可以采用numpy的矩阵函数一次性计算所有样本的代价函数。可以采用矩阵乘法(np.matmul())求和等方式（np.sum（））。利用你学到的线性代数知识，将其实现更改一下吧。\n",
    "\n",
    "在梯度更新时，我们保留了代价的历史信息。在参数的学习过程中，代价函数的变化过程你也可以作一个图来查看。观察最后得到的$J(\\theta)$的图像以及代价的变化过程，可以加深你的理解。在梯度下降的迭代中，我们设置终止条件为完成了固定的迭代次数，但是在迭代次数完成时，由于学习率等参数的设置，可能得到的参数并不是使得代价最低的值。你可以通过观察代价函数的变化过程，想办法调整学习率等参数或者改进程序，使得参数的取值为搜索到的最优结果。"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
